{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf2d4f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b9e912f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YF.download() has changed argument auto_adjust default to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price            Close        High         Low        Open    Volume\n",
      "Ticker            AAPL        AAPL        AAPL        AAPL      AAPL\n",
      "Date                                                                \n",
      "2024-05-17  188.986176  189.921802  188.299385  188.627851  41282900\n",
      "2024-05-20  190.150726  191.026635  188.130177  188.448695  44361300\n",
      "2024-05-21  191.454636  191.832856  190.031284  190.200491  42309400\n",
      "2024-05-22  190.011368  191.922444  189.384311  191.375001  34648500\n",
      "2024-05-23  186.010086  190.110903  185.761250  190.090992  51005900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "# Example: Download 1 year of data for Apple (AAPL)\n",
    "ticker = 'AAPL'\n",
    "data = yf.download(ticker, period='1y', interval='1d')  # You can also use start/end dates\n",
    "\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82ef0431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 250 entries, 2024-05-17 to 2025-05-16\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   (Close, AAPL)   250 non-null    float64\n",
      " 1   (High, AAPL)    250 non-null    float64\n",
      " 2   (Low, AAPL)     250 non-null    float64\n",
      " 3   (Open, AAPL)    250 non-null    float64\n",
      " 4   (Volume, AAPL)  250 non-null    int64  \n",
      "dtypes: float64(4), int64(1)\n",
      "memory usage: 11.7 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63d6ce30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79705088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Price</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>AAPL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-05-12</th>\n",
       "      <td>210.789993</td>\n",
       "      <td>211.270004</td>\n",
       "      <td>206.750000</td>\n",
       "      <td>210.970001</td>\n",
       "      <td>63775800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-05-13</th>\n",
       "      <td>212.929993</td>\n",
       "      <td>213.399994</td>\n",
       "      <td>209.000000</td>\n",
       "      <td>210.429993</td>\n",
       "      <td>51909300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-05-14</th>\n",
       "      <td>212.330002</td>\n",
       "      <td>213.940002</td>\n",
       "      <td>210.580002</td>\n",
       "      <td>212.429993</td>\n",
       "      <td>49325800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-05-15</th>\n",
       "      <td>211.449997</td>\n",
       "      <td>212.960007</td>\n",
       "      <td>209.539993</td>\n",
       "      <td>210.949997</td>\n",
       "      <td>45029500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-05-16</th>\n",
       "      <td>211.259995</td>\n",
       "      <td>212.570007</td>\n",
       "      <td>209.770004</td>\n",
       "      <td>212.360001</td>\n",
       "      <td>53659100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Price            Close        High         Low        Open    Volume\n",
       "Ticker            AAPL        AAPL        AAPL        AAPL      AAPL\n",
       "Date                                                                \n",
       "2025-05-12  210.789993  211.270004  206.750000  210.970001  63775800\n",
       "2025-05-13  212.929993  213.399994  209.000000  210.429993  51909300\n",
       "2025-05-14  212.330002  213.940002  210.580002  212.429993  49325800\n",
       "2025-05-15  211.449997  212.960007  209.539993  210.949997  45029500\n",
       "2025-05-16  211.259995  212.570007  209.770004  212.360001  53659100"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2557e0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(f\"{ticker}_stock_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "515584cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Yahoo Finance', 'date': '2025-05-18', 'text': \"Oops, something went wrong\\nAdd holdings \\nEpic Games is escalating its efforts to pressure Apple to allow its game Fortnite into its App Store, with a new court filing asking Judge Yvonne Gonzalez Rogers to require that Apple “accept any compliant version of Fortnite onto the U.S. storefront of the App Store.” Epic and Apple have been engaged in a years-long legal battle over Apple’s App Store policies, particularly the commissions Apple charges for in-app purchases. The Fortnite publisher scored a major victory last month when Judge Ro\\nApple could be looking to reinvent its Safari search engine through AI.  Apple (NASDAQ: AAPL) has faced a wave of headwinds this year, including the prospects of higher costs, thanks to President Trump's tariff policies.\\nWedbush analyst, Dan Ives anticipates that Apple Inc. (NASDAQ:AAPL) could increase its iPhone assembly production in India to 60%-65% by fall, depending on tariff negotiations. What Happened: On Friday, Ives posted his views on X that Apple could significantly ramp up its iPhone assembly in India in a best-case scenario. However, he also noted that the tech giant could revert to a China-driven iPhone strategy depending on the tariff situation and deal negotiations. He added that U.S. iPhone prod\\nApple and American Express are two of Berkshire Hathaway's largest investments.  Apple's push into services, in addition to Apple Intelligence, should lead to profitable growth for investors.  American Express is a resilient credit card brand that continues to adapt its offering to win over a new generation of consumers.\\nApple (NasdaqGS:AAPL) has experienced a 7% increase in its share price over the past month amid several noteworthy events. The company faces a new trial in a patent infringement case against Fintiv Inc., which could have created some market anxiety. However, this potential setback might have been counterbalanced by Apple's positive earnings report, showing revenue and net income growth. The announcement of a new $100 billion share repurchase plan and a dividend increase also likely bolstered...\\nApple's growing reliance on India for manufacturing iPhones is being attacked by Donald Trump.\\nTo perform complex tasks, like booking a flight, AI agents will need permissions to work on behalf of a person.\\nApple initially removed Fortnite from the App Store in 2020 after Epic Games tried to skirt commissions on in-app purchases.\\nMeta (META) reportedly has delayed the release of its latest artificial intelligence model, known internally as “Behemoth,” raising concerns among employees about the effectiveness and direction of the company’s massive AI investments.\\nEasing trade tensions have helped the stock market regain its footing.  Shares of Apple, AppLovin, and Super Micro Computer could be poised to rally higher.  The stock market has staged an impressive rebound following a turbulent last few months.\\nA food company is finding it tough to generate earnings growth.  The shift in revenue from products to services is leading to margin expansion at Apple.  The interesting thing about this list is that the two buys, Apple (NASDAQ: AAPL) and Pool Corp. (NASDAQ: POOL), have markedly higher valuations than the sell, Kraft Heinz (NASDAQ: KHC).\\nApple may get surpassed by Amazon in market cap by the end of this year.  Amazon has reason to grow its profits while Apple's may fall.  The two stocks' valuations indicate that Amazon has the potential to surpass Apple by the end of 2025.\\nShares of this Apple supplier have shot up impressively in the past month.  Investors can expect this chip stock to deliver impressive gains thanks to the secular growth opportunity in AI-enabled smartphones and PCs.  Apple supplier Cirrus Logic (NASDAQ: CRUS) has witnessed a big jump in its stock price in the past month, jumping 23% as of this writing after it emerged that its largest customer is getting a reprieve from the tariff turmoil.\\nPresident urges Apple to build domestically despite production costs\\nPresident Donald Trump said he told Apple Inc. (NASDAQ:AAPL) CEO Tim Cook about the tech giant’s manufacturing expansion in India, revealing details of their conversation during a news conference in Doha, Qatar, on Thursday. What Happened: “I had a little problem with Tim Cook yesterday... I said to him, ‘Tim, you’re my friend, we treated you very good... but now I hear you’re building all over India. I don’t want you building in India,” Trump stated. The exchange comes as Apple accelerates its\\nNvidia reached a $3.17 trillion valuation, just shy of Apple's $3.18 trillion at the close\\nIntel's 18A process is coming later this year. It's the company's biggest chance to catch back up with rival TSMC. But it's a risky bet.\\nThe company’s ‘aggressive push towards India production has been a very smart strategic move,’ Wedbush says.\\nPresident Trump wants Apple to make iPhones in the U.S. Wall Street analysts say that's not likely to happen.\\nEpic Games said 'Fortnite' will be offline on Apple devices because the iPhone maker blocked its app update. The move comes just weeks after Epic Games cheered a judge's ruling that limited the commissions that Apple makes through apps.\\nSign in to access your portfolio\"}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "def scrape_yahoo_news(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to fetch page: {response.status_code}\")\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Extract headline\n",
    "    title_tag = soup.find('h1')\n",
    "    title = title_tag.text.strip() if title_tag else \"No Title Found\"\n",
    "\n",
    "    # Extract content\n",
    "    paragraphs = soup.find_all('p')\n",
    "    article_text = \"\\n\".join([p.text for p in paragraphs])\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"date\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "        \"text\": article_text\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "url = \"https://finance.yahoo.com/quote/AAPL/news/?p=AAPL\"  # Replace with actual news article URL\n",
    "data = scrape_yahoo_news(url)\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad45f443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Yahoo Finance',\n",
       " 'date': '2025-05-18',\n",
       " 'text': \"Oops, something went wrong\\nAdd holdings \\nEpic Games is escalating its efforts to pressure Apple to allow its game Fortnite into its App Store, with a new court filing asking Judge Yvonne Gonzalez Rogers to require that Apple “accept any compliant version of Fortnite onto the U.S. storefront of the App Store.” Epic and Apple have been engaged in a years-long legal battle over Apple’s App Store policies, particularly the commissions Apple charges for in-app purchases. The Fortnite publisher scored a major victory last month when Judge Ro\\nApple could be looking to reinvent its Safari search engine through AI.  Apple (NASDAQ: AAPL) has faced a wave of headwinds this year, including the prospects of higher costs, thanks to President Trump's tariff policies.\\nWedbush analyst, Dan Ives anticipates that Apple Inc. (NASDAQ:AAPL) could increase its iPhone assembly production in India to 60%-65% by fall, depending on tariff negotiations. What Happened: On Friday, Ives posted his views on X that Apple could significantly ramp up its iPhone assembly in India in a best-case scenario. However, he also noted that the tech giant could revert to a China-driven iPhone strategy depending on the tariff situation and deal negotiations. He added that U.S. iPhone prod\\nApple and American Express are two of Berkshire Hathaway's largest investments.  Apple's push into services, in addition to Apple Intelligence, should lead to profitable growth for investors.  American Express is a resilient credit card brand that continues to adapt its offering to win over a new generation of consumers.\\nApple (NasdaqGS:AAPL) has experienced a 7% increase in its share price over the past month amid several noteworthy events. The company faces a new trial in a patent infringement case against Fintiv Inc., which could have created some market anxiety. However, this potential setback might have been counterbalanced by Apple's positive earnings report, showing revenue and net income growth. The announcement of a new $100 billion share repurchase plan and a dividend increase also likely bolstered...\\nApple's growing reliance on India for manufacturing iPhones is being attacked by Donald Trump.\\nTo perform complex tasks, like booking a flight, AI agents will need permissions to work on behalf of a person.\\nApple initially removed Fortnite from the App Store in 2020 after Epic Games tried to skirt commissions on in-app purchases.\\nMeta (META) reportedly has delayed the release of its latest artificial intelligence model, known internally as “Behemoth,” raising concerns among employees about the effectiveness and direction of the company’s massive AI investments.\\nEasing trade tensions have helped the stock market regain its footing.  Shares of Apple, AppLovin, and Super Micro Computer could be poised to rally higher.  The stock market has staged an impressive rebound following a turbulent last few months.\\nA food company is finding it tough to generate earnings growth.  The shift in revenue from products to services is leading to margin expansion at Apple.  The interesting thing about this list is that the two buys, Apple (NASDAQ: AAPL) and Pool Corp. (NASDAQ: POOL), have markedly higher valuations than the sell, Kraft Heinz (NASDAQ: KHC).\\nApple may get surpassed by Amazon in market cap by the end of this year.  Amazon has reason to grow its profits while Apple's may fall.  The two stocks' valuations indicate that Amazon has the potential to surpass Apple by the end of 2025.\\nShares of this Apple supplier have shot up impressively in the past month.  Investors can expect this chip stock to deliver impressive gains thanks to the secular growth opportunity in AI-enabled smartphones and PCs.  Apple supplier Cirrus Logic (NASDAQ: CRUS) has witnessed a big jump in its stock price in the past month, jumping 23% as of this writing after it emerged that its largest customer is getting a reprieve from the tariff turmoil.\\nPresident urges Apple to build domestically despite production costs\\nPresident Donald Trump said he told Apple Inc. (NASDAQ:AAPL) CEO Tim Cook about the tech giant’s manufacturing expansion in India, revealing details of their conversation during a news conference in Doha, Qatar, on Thursday. What Happened: “I had a little problem with Tim Cook yesterday... I said to him, ‘Tim, you’re my friend, we treated you very good... but now I hear you’re building all over India. I don’t want you building in India,” Trump stated. The exchange comes as Apple accelerates its\\nNvidia reached a $3.17 trillion valuation, just shy of Apple's $3.18 trillion at the close\\nIntel's 18A process is coming later this year. It's the company's biggest chance to catch back up with rival TSMC. But it's a risky bet.\\nThe company’s ‘aggressive push towards India production has been a very smart strategic move,’ Wedbush says.\\nPresident Trump wants Apple to make iPhones in the U.S. Wall Street analysts say that's not likely to happen.\\nEpic Games said 'Fortnite' will be offline on Apple devices because the iPhone maker blocked its app update. The move comes just weeks after Epic Games cheered a judge's ruling that limited the commissions that Apple makes through apps.\\nSign in to access your portfolio\"}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5fdc1585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://finance.yahoo.com/quote/AAPL/news?p=AAPL&offset=0&count=10\n",
      "Fetching: https://finance.yahoo.com/quote/AAPL/news?p=AAPL&offset=10&count=10\n",
      "Fetching: https://finance.yahoo.com/quote/AAPL/news?p=AAPL&offset=20&count=10\n",
      "Fetching: https://finance.yahoo.com/quote/AAPL/news?p=AAPL&offset=30&count=10\n",
      "Fetching: https://finance.yahoo.com/quote/AAPL/news?p=AAPL&offset=40&count=10\n",
      "Fetching: https://finance.yahoo.com/quote/AAPL/news?p=AAPL&offset=50&count=10\n",
      "Fetching: https://finance.yahoo.com/quote/AAPL/news?p=AAPL&offset=60&count=10\n",
      "Fetching: https://finance.yahoo.com/quote/AAPL/news?p=AAPL&offset=70&count=10\n",
      "Fetching: https://finance.yahoo.com/quote/AAPL/news?p=AAPL&offset=80&count=10\n",
      "Fetching: https://finance.yahoo.com/quote/AAPL/news?p=AAPL&offset=90&count=10\n",
      "Fetching: https://finance.yahoo.com/quote/AAPL/news?p=AAPL&offset=100&count=10\n",
      "Fetching: https://finance.yahoo.com/quote/AAPL/news?p=AAPL&offset=110&count=10\n",
      "Fetching: https://finance.yahoo.com/quote/AAPL/news?p=AAPL&offset=120&count=10\n",
      "Fetching: https://finance.yahoo.com/quote/AAPL/news?p=AAPL&offset=130&count=10\n",
      "Fetching: https://finance.yahoo.com/quote/AAPL/news?p=AAPL&offset=140&count=10\n",
      "Fetching: https://finance.yahoo.com/quote/AAPL/news?p=AAPL&offset=150&count=10\n",
      "Fetching: https://finance.yahoo.com/quote/AAPL/news?p=AAPL&offset=160&count=10\n",
      "Fetching: https://finance.yahoo.com/quote/AAPL/news?p=AAPL&offset=170&count=10\n",
      "Fetching: https://finance.yahoo.com/quote/AAPL/news?p=AAPL&offset=180&count=10\n",
      "Fetching: https://finance.yahoo.com/quote/AAPL/news?p=AAPL&offset=190&count=10\n",
      "Fetching: https://finance.yahoo.com/quote/AAPL/news?p=AAPL&offset=200&count=10\n",
      "Fetching: https://finance.yahoo.com/quote/AAPL/news?p=AAPL&offset=210&count=10\n",
      "Fetching: https://finance.yahoo.com/quote/AAPL/news?p=AAPL&offset=220&count=10\n",
      "Fetching: https://finance.yahoo.com/quote/AAPL/news?p=AAPL&offset=230&count=10\n",
      "Fetching: https://finance.yahoo.com/quote/AAPL/news?p=AAPL&offset=240&count=10\n",
      "Fetching: https://finance.yahoo.com/quote/AAPL/news?p=AAPL&offset=250&count=10\n",
      "Fetching: https://finance.yahoo.com/quote/AAPL/news?p=AAPL&offset=260&count=10\n",
      "Fetching: https://finance.yahoo.com/quote/AAPL/news?p=AAPL&offset=270&count=10\n",
      "Fetching: https://finance.yahoo.com/quote/AAPL/news?p=AAPL&offset=280&count=10\n",
      "Fetching: https://finance.yahoo.com/quote/AAPL/news?p=AAPL&offset=290&count=10\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "def get_yahoo_news(ticker=\"AAPL\", max_pages=10):\n",
    "    base_url = f\"https://finance.yahoo.com/quote/{ticker}/news?p={ticker}\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0\"\n",
    "    }\n",
    "\n",
    "    articles = []\n",
    "\n",
    "    for page in range(0, max_pages):\n",
    "        url = f\"{base_url}&offset={page * 10}&count=10\"\n",
    "        print(f\"Fetching: {url}\")\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        for item in soup.select('li.js-stream-content'):\n",
    "            try:\n",
    "                title = item.select_one(\"h3\").text.strip()\n",
    "                link = \"https://finance.yahoo.com\" + item.a['href']\n",
    "                date_str = item.select_one('span.Cf span').text.strip()\n",
    "                articles.append({\n",
    "                    \"title\": title,\n",
    "                    \"link\": link,\n",
    "                    \"date\": date_str\n",
    "                })\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        time.sleep(2)  # polite crawling\n",
    "\n",
    "    return articles\n",
    "\n",
    "# Usage\n",
    "news_data = get_yahoo_news(\"AAPL\", max_pages=30)  # Adjust max_pages as needed\n",
    "for article in news_data:\n",
    "    print(article)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993f455c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "\n",
    "def parse_relative_date(date_str):\n",
    "    \"\"\"Parse relative date strings like '2 hours ago', '3 days ago'.\"\"\"\n",
    "    now = datetime.now()\n",
    "    if \"hour\" in date_str:\n",
    "        hours = int(re.search(r\"(\\d+)\", date_str).group(1))\n",
    "        return now - timedelta(hours=hours)\n",
    "    elif \"minute\" in date_str:\n",
    "        minutes = int(re.search(r\"(\\d+)\", date_str).group(1))\n",
    "        return now - timedelta(minutes=minutes)\n",
    "    elif \"day\" in date_str:\n",
    "        days = int(re.search(r\"(\\d+)\", date_str).group(1))\n",
    "        return now - timedelta(days=days)\n",
    "    elif \"month\" in date_str:\n",
    "        months = int(re.search(r\"(\\d+)\", date_str).group(1))\n",
    "        return now - timedelta(days=30*months)\n",
    "    else:\n",
    "        # fallback for exact date strings or unknown formats\n",
    "        try:\n",
    "            return datetime.strptime(date_str, \"%b %d, %Y\")\n",
    "        except:\n",
    "            return now  # just use now if parsing fails\n",
    "\n",
    "def get_yahoo_news_2months(ticker=\"AAPL\", max_pages=100):\n",
    "    base_url = f\"https://finance.yahoo.com/quote/{ticker}/news?p={ticker}\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    articles = []\n",
    "    cutoff_date = datetime.now() - timedelta(days=60)\n",
    "\n",
    "    for page in range(max_pages):\n",
    "        url = f\"{base_url}&offset={page * 10}&count=10\"\n",
    "        print(f\"Fetching: {url}\")\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        older_article_found = False\n",
    "\n",
    "        for item in soup.select('li.js-stream-content'):\n",
    "            try:\n",
    "                title_tag = item.select_one(\"h3\")\n",
    "                if not title_tag:\n",
    "                    continue\n",
    "                title = title_tag.text.strip()\n",
    "                link = \"https://finance.yahoo.com\" + item.a['href']\n",
    "\n",
    "                date_tag = item.select_one('div.Cf span:nth-child(2)')\n",
    "                date_str = date_tag.text.strip() if date_tag else \"N/A\"\n",
    "                article_date = parse_relative_date(date_str)\n",
    "\n",
    "                if article_date < cutoff_date:\n",
    "                    older_article_found = True\n",
    "                    break  # stop processing older articles\n",
    "\n",
    "                articles.append({\n",
    "                    \"title\": title,\n",
    "                    \"link\": link,\n",
    "                    \"date\": article_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(\"Error parsing an article:\", e)\n",
    "                continue\n",
    "\n",
    "        if older_article_found:\n",
    "            print(\"Reached articles older than 2 months. Stopping.\")\n",
    "            break\n",
    "\n",
    "        time.sleep(2)\n",
    "\n",
    "    return articles\n",
    "\n",
    "# Usage:\n",
    "news_data = get_yahoo_news_2months(\"AAPL\")\n",
    "print(f\"Fetched {len(news_data)} articles from past 2 months.\")\n",
    "for article in news_data[:5]:\n",
    "    print(article)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75a792f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://finance.yahoo.com/quote/AAPL/news?p=AAPL&offset=0&count=10\n",
      "Fetching: https://finance.yahoo.com/quote/AAPL/news?p=AAPL&offset=10&count=10\n",
      "Fetching: https://finance.yahoo.com/quote/AAPL/news?p=AAPL&offset=20&count=10\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='finance.yahoo.com', port=443): Max retries exceeded with url: /quote/AAPL/news?p=AAPL&offset=20&count=10 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002CE30CC7790>: Failed to establish a new connection: [WinError 10051] A socket operation was attempted to an unreachable network'))",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vivek gupta\\Desktop\\Stock Price Prediction\\my_env\\Lib\\site-packages\\urllib3\\connection.py:198\u001b[39m, in \u001b[36mHTTPConnection._new_conn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     sock = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m socket.gaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vivek gupta\\Desktop\\Stock Price Prediction\\my_env\\Lib\\site-packages\\urllib3\\util\\connection.py:85\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, socket_options)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     87\u001b[39m     \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vivek gupta\\Desktop\\Stock Price Prediction\\my_env\\Lib\\site-packages\\urllib3\\util\\connection.py:73\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, socket_options)\u001b[39m\n\u001b[32m     72\u001b[39m     sock.bind(source_address)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m sock.connect(sa)\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[31mOSError\u001b[39m: [WinError 10051] A socket operation was attempted to an unreachable network",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mNewConnectionError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vivek gupta\\Desktop\\Stock Price Prediction\\my_env\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vivek gupta\\Desktop\\Stock Price Prediction\\my_env\\Lib\\site-packages\\urllib3\\connectionpool.py:488\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    487\u001b[39m         new_e = _wrap_proxy_error(new_e, conn.proxy.scheme)\n\u001b[32m--> \u001b[39m\u001b[32m488\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[32m    490\u001b[39m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[32m    491\u001b[39m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vivek gupta\\Desktop\\Stock Price Prediction\\my_env\\Lib\\site-packages\\urllib3\\connectionpool.py:464\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    463\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vivek gupta\\Desktop\\Stock Price Prediction\\my_env\\Lib\\site-packages\\urllib3\\connectionpool.py:1093\u001b[39m, in \u001b[36mHTTPSConnectionPool._validate_conn\u001b[39m\u001b[34m(self, conn)\u001b[39m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m conn.is_closed:\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m     \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vivek gupta\\Desktop\\Stock Price Prediction\\my_env\\Lib\\site-packages\\urllib3\\connection.py:704\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    703\u001b[39m sock: socket.socket | ssl.SSLSocket\n\u001b[32m--> \u001b[39m\u001b[32m704\u001b[39m \u001b[38;5;28mself\u001b[39m.sock = sock = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m server_hostname: \u001b[38;5;28mstr\u001b[39m = \u001b[38;5;28mself\u001b[39m.host\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vivek gupta\\Desktop\\Stock Price Prediction\\my_env\\Lib\\site-packages\\urllib3\\connection.py:213\u001b[39m, in \u001b[36mHTTPConnection._new_conn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[32m    214\u001b[39m         \u001b[38;5;28mself\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    215\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    217\u001b[39m sys.audit(\u001b[33m\"\u001b[39m\u001b[33mhttp.client.connect\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m.host, \u001b[38;5;28mself\u001b[39m.port)\n",
      "\u001b[31mNewConnectionError\u001b[39m: <urllib3.connection.HTTPSConnection object at 0x000002CE30CC7790>: Failed to establish a new connection: [WinError 10051] A socket operation was attempted to an unreachable network",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mMaxRetryError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vivek gupta\\Desktop\\Stock Price Prediction\\my_env\\Lib\\site-packages\\requests\\adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vivek gupta\\Desktop\\Stock Price Prediction\\my_env\\Lib\\site-packages\\urllib3\\connectionpool.py:841\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    839\u001b[39m     new_e = ProtocolError(\u001b[33m\"\u001b[39m\u001b[33mConnection aborted.\u001b[39m\u001b[33m\"\u001b[39m, new_e)\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m retries = \u001b[43mretries\u001b[49m\u001b[43m.\u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m=\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    844\u001b[39m retries.sleep()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vivek gupta\\Desktop\\Stock Price Prediction\\my_env\\Lib\\site-packages\\urllib3\\util\\retry.py:519\u001b[39m, in \u001b[36mRetry.increment\u001b[39m\u001b[34m(self, method, url, response, error, _pool, _stacktrace)\u001b[39m\n\u001b[32m    518\u001b[39m     reason = error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[32m--> \u001b[39m\u001b[32m519\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    521\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mIncremented Retry for (url=\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m, url, new_retry)\n",
      "\u001b[31mMaxRetryError\u001b[39m: HTTPSConnectionPool(host='finance.yahoo.com', port=443): Max retries exceeded with url: /quote/AAPL/news?p=AAPL&offset=20&count=10 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002CE30CC7790>: Failed to establish a new connection: [WinError 10051] A socket operation was attempted to an unreachable network'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mConnectionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Example usage:  \u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m news_data = \u001b[43mget_yahoo_news\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mAAPL\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_pages\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m news_data:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTitle: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marticle[\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mget_yahoo_news\u001b[39m\u001b[34m(ticker, max_pages)\u001b[39m\n\u001b[32m     15\u001b[39m url = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m&offset=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;250m \u001b[39m*\u001b[38;5;250m \u001b[39m\u001b[32m10\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m&count=10\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFetching: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m soup = BeautifulSoup(response.content, \u001b[33m\"\u001b[39m\u001b[33mhtml.parser\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m soup.select(\u001b[33m'\u001b[39m\u001b[33mli.js-stream-content\u001b[39m\u001b[33m'\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vivek gupta\\Desktop\\Stock Price Prediction\\my_env\\Lib\\site-packages\\requests\\api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vivek gupta\\Desktop\\Stock Price Prediction\\my_env\\Lib\\site-packages\\requests\\api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vivek gupta\\Desktop\\Stock Price Prediction\\my_env\\Lib\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vivek gupta\\Desktop\\Stock Price Prediction\\my_env\\Lib\\site-packages\\requests\\sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vivek gupta\\Desktop\\Stock Price Prediction\\my_env\\Lib\\site-packages\\requests\\adapters.py:700\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    696\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e.reason, _SSLError):\n\u001b[32m    697\u001b[39m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[32m    698\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request=request)\n\u001b[32m--> \u001b[39m\u001b[32m700\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request=request)\n\u001b[32m    702\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    703\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request=request)\n",
      "\u001b[31mConnectionError\u001b[39m: HTTPSConnectionPool(host='finance.yahoo.com', port=443): Max retries exceeded with url: /quote/AAPL/news?p=AAPL&offset=20&count=10 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002CE30CC7790>: Failed to establish a new connection: [WinError 10051] A socket operation was attempted to an unreachable network'))"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def extract_article_text(url):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        # Different websites structure article body differently.\n",
    "        # For Yahoo Finance, the main article text is usually inside <div> tags with 'caas-body' class\n",
    "        content_div = soup.find('div', class_='caas-body')\n",
    "        if not content_div:\n",
    "            # fallback: try main tag or article tag\n",
    "            content_div = soup.find('article') or soup.find('main')\n",
    "        \n",
    "        if content_div:\n",
    "            paragraphs = content_div.find_all('p')\n",
    "            article_text = \"\\n\".join(p.get_text() for p in paragraphs)\n",
    "            return article_text.strip()\n",
    "        else:\n",
    "            return \"\"  # Could not find article body\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch article {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Example usage:  \n",
    "news_data = get_yahoo_news(\"AAPL\", max_pages=3)\n",
    "\n",
    "for article in news_data:\n",
    "    print(f\"Title: {article['title']}\")\n",
    "    print(f\"Date: {article['date']}\")\n",
    "    print(f\"Link: {article['link']}\")\n",
    "    \n",
    "    # Extract full article text\n",
    "    article_text = extract_article_text(article['link'])\n",
    "    print(f\"Article text preview:\\n{article_text[:500]}\")  # print first 500 chars\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    time.sleep(1)  # polite delay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf0e50cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import quote\n",
    "import time\n",
    "\n",
    "def scrape_google_news(query, start_date, end_date, pages=5):\n",
    "    all_articles = []\n",
    "    for page in range(pages):\n",
    "        start = page * 10\n",
    "        query_encoded = quote(query)\n",
    "        url = f\"https://www.google.com/search?q={query_encoded}&hl=en&tbm=nws&tbs=cdr:1,cd_min:{start_date},cd_max:{end_date}&start={start}\"\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0\"\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        results = soup.select(\"div.dbsr\")\n",
    "\n",
    "        for result in results:\n",
    "            try:\n",
    "                title = result.select_one(\"div.JheGif.nDgy9d\").text\n",
    "                link = result.a['href']\n",
    "                source_time = result.select_one(\".XTjFC.WF4CUc\").text\n",
    "                snippet = result.select_one(\".Y3v8qd\").text\n",
    "                all_articles.append({\n",
    "                    \"title\": title,\n",
    "                    \"link\": link,\n",
    "                    \"source_time\": source_time,\n",
    "                    \"snippet\": snippet\n",
    "                })\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        time.sleep(2)  # be polite\n",
    "\n",
    "    return all_articles\n",
    "\n",
    "# Example usage\n",
    "articles = scrape_google_news(\"Apple stock\", \"3/17/2024\", \"5/17/2024\", pages=10)\n",
    "\n",
    "# Print sample\n",
    "for article in articles[:5]:\n",
    "    print(article)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2cacad9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in articles[:5]:\n",
    "    print(article)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "502191e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No results found or blocked by Google. Try again with VPN or proxy.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import quote\n",
    "import time\n",
    "\n",
    "def scrape_google_news(query, start_date, end_date, pages=5):\n",
    "    all_articles = []\n",
    "    for page in range(pages):\n",
    "        start = page * 10\n",
    "        query_encoded = quote(query)\n",
    "        url = f\"https://www.google.com/search?q={query_encoded}&hl=en&tbm=nws&tbs=cdr:1,cd_min:{start_date},cd_max:{end_date}&start={start}\"\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        results = soup.select(\"div.Gx5Zad.fP1Qef.xpd.EtOod.pkphOe\")\n",
    "\n",
    "        if not results:\n",
    "            print(\"No results found or blocked by Google. Try again with VPN or proxy.\")\n",
    "            break\n",
    "\n",
    "        for result in results:\n",
    "            try:\n",
    "                title_tag = result.select_one(\"div.BNeawe.vvjwJb.AP7Wnd\")\n",
    "                snippet_tag = result.select_one(\"div.BNeawe.s3v9rd.AP7Wnd\")\n",
    "                link_tag = result.find(\"a\")\n",
    "\n",
    "                title = title_tag.text if title_tag else \"No title\"\n",
    "                snippet = snippet_tag.text if snippet_tag else \"No snippet\"\n",
    "                link = link_tag['href'] if link_tag else \"No link\"\n",
    "                all_articles.append({\n",
    "                    \"title\": title,\n",
    "                    \"link\": link,\n",
    "                    \"snippet\": snippet\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(\"Error parsing result:\", e)\n",
    "\n",
    "        time.sleep(1)  # Avoid being blocked\n",
    "\n",
    "    return all_articles\n",
    "\n",
    "# Example usage\n",
    "articles = scrape_google_news(\"Apple stock\", \"3/17/2024\", \"5/17/2024\", pages=3)\n",
    "\n",
    "# Print results\n",
    "for i, article in enumerate(articles[:5], 1):\n",
    "    print(f\"{i}. {article['title']}\")\n",
    "    print(f\"Link: {article['link']}\")\n",
    "    print(f\"Snippet: {article['snippet']}\")\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "27b9b69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "api_key = 'your_gnews_api_key'\n",
    "query = 'Apple stock'\n",
    "start_date = '2024-03-17'\n",
    "end_date = '2024-05-17'\n",
    "\n",
    "api_key = '78312341db9780e245fee122357261d7'\n",
    "url = f\"https://gnews.io/api/v4/search?q={query}&from={start_date}&to={end_date}&lang=en&token={api_key}\"\n",
    "\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "articles = data.get('articles', [])\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame([{\n",
    "    \"title\": a[\"title\"],\n",
    "    \"description\": a[\"description\"],\n",
    "    \"publishedAt\": a[\"publishedAt\"],\n",
    "    \"url\": a[\"url\"]\n",
    "} for a in articles])\n",
    "\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0afeef7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
